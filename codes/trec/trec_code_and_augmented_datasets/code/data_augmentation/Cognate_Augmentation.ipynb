{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f0b7780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (C:\\Users\\DELL\\.cache\\huggingface\\datasets\\trec\\default\\1.1.0\\751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 668.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 5452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\DELL\\.cache\\huggingface\\datasets\\trec\\default\\1.1.0\\751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9\\cache-bc761497303cb1ab.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\DELL\\.cache\\huggingface\\datasets\\trec\\default\\1.1.0\\751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9\\cache-d432b7eeb4042bcd.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I transport files from one computer to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Who are the two sons of Ozzie and Harriet Nels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What does pH stand for ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How many people does Honda employ in the U.S. ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>What newspaper returned a Pulitzer Prize for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>1</td>\n",
       "      <td>What does Final Four refer to in the sports wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>3</td>\n",
       "      <td>What is Nicholas Cage 's occupation ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>1</td>\n",
       "      <td>In order from the top , the four stripes on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>3</td>\n",
       "      <td>What kind of business is 7-Eleven ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>1</td>\n",
       "      <td>What soft drink held a national flavor poll in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  How can I transport files from one computer to...\n",
       "1         3  Who are the two sons of Ozzie and Harriet Nels...\n",
       "2         2                           What does pH stand for ?\n",
       "3         4    How many people does Honda employ in the U.S. ?\n",
       "4         1  What newspaper returned a Pulitzer Prize for t...\n",
       "...     ...                                                ...\n",
       "4495      1  What does Final Four refer to in the sports wo...\n",
       "4496      3              What is Nicholas Cage 's occupation ?\n",
       "4497      1  In order from the top , the four stripes on a ...\n",
       "4498      3                What kind of business is 7-Eleven ?\n",
       "4499      1  What soft drink held a national flavor poll in...\n",
       "\n",
       "[4500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset('trec')\n",
    "#display(dataset)\n",
    "dataset = dataset.remove_columns(\"label-fine\")\n",
    "#display(dataset)\n",
    "dataset = dataset.rename_column(\"label-coarse\", \"label\")\n",
    "display(dataset)\n",
    "\n",
    "\n",
    "train_validation_ds = dataset['train'].shuffle(seed=42).select([i for i in list(range(5000))])\n",
    "train_ds = Dataset.from_dict(train_validation_ds[0:4500])\n",
    "validation_ds = Dataset.from_dict(train_validation_ds[4500:5000])\n",
    "test_ds = dataset['test'].shuffle(seed=42).select([i for i in list(range(500))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(train_ds)\n",
    "train_df = pd.DataFrame(train_ds)\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e8f0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordnet part\n",
    "import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from senticnet.senticnet import SenticNet\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from lemminflect import getInflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c18177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating synonyms list using wordnet\n",
    "def synonyms_list_creation(target_word):\n",
    "    synonyms_list = []\n",
    "    try:\n",
    "        for syn in wordnet.synsets(target_word):\n",
    "            for lm in syn.lemmas():\n",
    "                synonyms_list.append(lm.name())\n",
    "        #print (set(synonyms_list))\n",
    "        synonyms_list = list(set(synonyms_list))\n",
    "        #transform _ to space\n",
    "        synonyms_list = [s.replace('_', ' ') for s in synonyms_list]\n",
    "\n",
    "\n",
    "        synonyms_list\n",
    "        synonyms_list = [s for s in synonyms_list if ' ' not in s]\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return synonyms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d1e0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#senticnet part\n",
    "senticnet_df = pd.read_excel(r'C:\\Users\\DELL\\Text_Augmentation\\senticnet\\senticnet.xlsx',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e99c7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               INTROSPECTION  TEMPER  ATTITUDE  SENSITIVITY PRIMARY EMOTION  \\\n",
      "CONCEPT                                                                       \n",
      "abandon               -0.165     0.0     0.000        0.000     #melancholy   \n",
      "abase                  0.000     0.0    -0.890        0.000       #loathing   \n",
      "abash                  0.000     0.0    -0.341       -0.442           #fear   \n",
      "abashment              0.000     0.0    -0.329        0.000        #dislike   \n",
      "abate                  0.000     0.0    -0.339       -0.551           #fear   \n",
      "...                      ...     ...       ...          ...             ...   \n",
      "zotob worm             0.000     0.0     0.000       -0.958         #terror   \n",
      "zotob wrestle          0.000     0.0     0.000       -0.962         #terror   \n",
      "zotob wriggle          0.000     0.0     0.000       -0.918         #terror   \n",
      "zotob writhe           0.000     0.0     0.000       -0.979         #terror   \n",
      "zz                     0.000     0.0    -0.182        0.000        #dislike   \n",
      "\n",
      "              SECONDAY EMOTION POLARITY VALUE  POLARITY INTENSITY   SEMANTICS  \\\n",
      "CONCEPT                                                                         \n",
      "abandon                   None       negative              -0.165  melancholy   \n",
      "abase                     None       negative              -0.890       loath   \n",
      "abash                 #disgust       negative              -0.392     chagrin   \n",
      "abashment                 None       negative              -0.329     dislike   \n",
      "abate                 #disgust       negative              -0.445   embarrass   \n",
      "...                        ...            ...                 ...         ...   \n",
      "zotob worm                None       negative              -0.958    bulldoze   \n",
      "zotob wrestle             None       negative              -0.962    teardown   \n",
      "zotob wriggle             None       negative              -0.918  knock_down   \n",
      "zotob writhe              None       negative              -0.979  demolition   \n",
      "zz                        None       negative              -0.182   good_rest   \n",
      "\n",
      "              Unnamed: 10 Unnamed: 11 Unnamed: 12  Unnamed: 13  \n",
      "CONCEPT                                                         \n",
      "abandon           pensive   emptiness  down_heart    nostalgia  \n",
      "abase           revulsion    contempt  repugnance   abhorrence  \n",
      "abash           chagrined   embarrass       abash   disconcert  \n",
      "abashment        distaste   rejection   disbelief  disapproval  \n",
      "abate           chagrined  disconcert     chagrin        abash  \n",
      "...                   ...         ...         ...          ...  \n",
      "zotob worm     demolition    teardown    destruct   knock_down  \n",
      "zotob wrestle    bulldoze  knock_down  demolition     destruct  \n",
      "zotob wriggle    teardown  demolition    destruct     bulldoze  \n",
      "zotob writhe   knock_down    teardown    destruct     bulldoze  \n",
      "zz              rest_well   stay_well      health     sleeping  \n",
      "\n",
      "[152319 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#transform _ to space\n",
    "\n",
    "senticnet_df['CONCEPT'] = senticnet_df['CONCEPT'].str.replace('_', ' ')\n",
    "\n",
    "\n",
    "senticnet_df.set_index('CONCEPT',inplace=True)\n",
    "\n",
    "print(senticnet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64260337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating similar words list using senticnet\n",
    "def similar_polarity_words_list_creation(target_word):\n",
    "    if target_word in senticnet_df.index:\n",
    "        INTROSPECTION = senticnet_df.loc[target_word, 'INTROSPECTION']\n",
    "        TEMPER = senticnet_df.loc[target_word, 'TEMPER']\n",
    "        ATTITUDE = senticnet_df.loc[target_word, 'ATTITUDE']\n",
    "        SENSITIVITY = senticnet_df.loc[target_word, 'SENSITIVITY']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        extracted_df = senticnet_df[(senticnet_df['INTROSPECTION'] == INTROSPECTION) & \n",
    "                                   (senticnet_df['TEMPER'] == TEMPER) & (senticnet_df['ATTITUDE'] == ATTITUDE) & \n",
    "                                    (senticnet_df['SENSITIVITY'] == SENSITIVITY)]\n",
    "\n",
    "\n",
    "        similar_polarity_words_list = extracted_df.index.values.tolist()\n",
    "        similar_polarity_words_list = [s for s in similar_polarity_words_list if ' ' not in s]\n",
    "    \n",
    "    else:\n",
    "        similar_polarity_words_list = []\n",
    "    \n",
    "    return similar_polarity_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6d8998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from textattack.transformations import WordSwap\n",
    "class Cognate_WordSwap(WordSwap):\n",
    "    \n",
    "    \n",
    "    # We don't need a constructor, since our class doesn't require any parameters.\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        synonyms_list = synonyms_list_creation(word)\n",
    "        cognate_list = []\n",
    "        try:\n",
    "            cognate_list.append(similar_polarity_words_list_creation(word))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        cognate_list.append(synonyms_list)\n",
    "        cognate_list = list(itertools.chain.from_iterable(cognate_list))\n",
    "        cognate_list = list(set(cognate_list))\n",
    "\n",
    "        return cognate_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6abebf1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 1.16MB/s]\n",
      "Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "Use device: gpu\n",
      "Loading: tokenize\n",
      "Loading: pos\n",
      "Done loading processors!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.pre_transformation import RepeatModification\n",
    "from textattack.constraints.pre_transformation import StopwordModification\n",
    "from textattack.constraints.grammaticality.part_of_speech import PartOfSpeech\n",
    "from textattack.constraints.semantics.bert_score import BERTScore\n",
    "from textattack.augmentation import Augmenter\n",
    "\n",
    "\n",
    "transformation = Cognate_WordSwap()\n",
    "\n",
    "# Set up constraints\n",
    "constraints = [RepeatModification(), StopwordModification(),\n",
    "               PartOfSpeech(tagger_type='stanza', tagset='universal', allow_verb_noun_swap=False, compare_against_original=True, language_nltk='eng', language_stanza='en'),\n",
    "                BERTScore(min_bert_score = 0.9, model_name='bert-base-uncased', num_layers=None, score_type='f1', compare_against_original=True)]\n",
    "# Create augmenter with specified parameters\n",
    "augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0.1, transformations_per_example=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca955c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment!\n",
    "# additional parameters can be modified if not during initiation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def results_list_creation(s):\n",
    "\n",
    "    try:\n",
    "        results = augmenter.augment(s)\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        results = ['Error occured in text generation .']\n",
    "    \n",
    "    return results\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from concurrent import futures\n",
    "\n",
    "time_sta = time.perf_counter()\n",
    "results_list = []\n",
    "\n",
    "#print(list(train_df['text']))\n",
    "\n",
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    results_list_itera = executor.map(results_list_creation, list(train_df['text'])) \n",
    "\n",
    "for results in results_list_itera:\n",
    "    results_list.append(results)\n",
    "\n",
    "time_end = time.perf_counter()\n",
    "tim = time_end - time_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1341790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tim/3600)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43333379",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef2807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "original_sentences_list = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    number_of_generated_sentences_per_original_sentence = len(results_list[i])\n",
    "    original_sentences = [list(train_df['text'])[i] for j in range(number_of_generated_sentences_per_original_sentence)]\n",
    "    original_sentences_list.append(original_sentences)\n",
    "display(original_sentences_list)\n",
    "\n",
    "original_sentences_list_1d = list(itertools.chain.from_iterable(original_sentences_list))\n",
    "results_list_1d = list(itertools.chain.from_iterable(results_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5967a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "augmented_data = {'original_text':original_sentences_list_1d,'perturbed_text':results_list_1d}\n",
    "augmented_data_df = pd.DataFrame(augmented_data)\n",
    "display(augmented_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17d4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove augmented_data_df['original_text'] == augmented_data_df['perturbed_text']\n",
    "target = augmented_data_df.index[augmented_data_df['original_text'] == augmented_data_df['perturbed_text']]\n",
    "augmented_data_df = augmented_data_df.drop(target)\n",
    "augmented_data_df = augmented_data_df.reset_index(drop=True)\n",
    "display(augmented_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dad10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file\n",
    "augmented_data_df.to_csv( r'C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\augumented_dataset\\Cognate\\with_pos\\allow_verb_noun_swap=False_bert_score=0.9\\run3\\dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f80f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
