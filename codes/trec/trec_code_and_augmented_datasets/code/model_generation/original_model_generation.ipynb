{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c697ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\envs\\ta\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72de0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 5.50kB [00:00, 2.80MB/s]                                                                                  \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset trec/default to C:\\Users\\DELL\\.cache\\huggingface\\datasets\\trec\\default\\1.1.0\\751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|                                                                          | 0.00/336k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading:   2%|█▌                                                               | 8.19k/336k [00:00<00:10, 30.4kB/s]\u001b[A\n",
      "Downloading:  12%|███████▉                                                         | 41.0k/336k [00:00<00:03, 82.5kB/s]\u001b[A\n",
      "Downloading:  31%|████████████████████▊                                              | 104k/336k [00:00<00:01, 151kB/s]\u001b[A\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 336k/336k [00:01<00:00, 305kB/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:03<00:03,  3.07s/it]\n",
      "Downloading:   0%|                                                                         | 0.00/23.4k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 23.4k/23.4k [00:00<00:00, 83.6kB/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.54s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1020.26it/s]\n",
      "                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trec downloaded and prepared to C:\\Users\\DELL\\.cache\\huggingface\\datasets\\trec\\default\\1.1.0\\751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 403.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 5452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "dataset = load_dataset('trec')\n",
    "#display(dataset)\n",
    "dataset = dataset.remove_columns(\"label-fine\")\n",
    "#display(dataset)\n",
    "dataset = dataset.rename_column(\"label-coarse\", \"label\")\n",
    "display(dataset)\n",
    "\n",
    "\n",
    "train_validation_ds = dataset['train'].shuffle(seed=42).select([i for i in list(range(5000))])\n",
    "train_ds = Dataset.from_dict(train_validation_ds[0:4500])\n",
    "validation_ds = Dataset.from_dict(train_validation_ds[4500:5000])\n",
    "test_ds = dataset['test'].shuffle(seed=42).select([i for i in list(range(500))])\n",
    "\n",
    "display(train_ds)\n",
    "display(validation_ds)\n",
    "display(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95403ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 3, 2, 4, 1],\n",
       " 'text': ['How can I transport files from one computer to another ?',\n",
       "  'Who are the two sons of Ozzie and Harriet Nelson ?',\n",
       "  'What does pH stand for ?',\n",
       "  'How many people does Honda employ in the U.S. ?',\n",
       "  \"What newspaper returned a Pulitzer Prize for the fraudulent story Jimmy 's World ?\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ec0ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 5, 4, 1, 1],\n",
       " 'text': ['Which medium is Hamblen the first singing cowboy in ?',\n",
       "  'What country was the setting of You Only Live Twice ?',\n",
       "  'How many double-word-score spaces are there on a Scrabble Crossword Game board ?',\n",
       "  'What is the nickname of the Cleveland Indians ?',\n",
       "  'What drug is often used to treat AIDS patients ?']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af3c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [4, 0, 1, 4, 1],\n",
       " 'text': ['What is the population of Venezuela ?',\n",
       "  'What does target heart rate mean ?',\n",
       "  'What currency does Luxembourg use ?',\n",
       "  'How long did Rip Van Winkle sleep ?',\n",
       "  'Material called linen is made from what plant ?']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6644348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5267bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x0000015FBED21E50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 29.15ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 85.50ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.62ba/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#defining model:\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          Trainer, EarlyStoppingCallback,\n",
    "                          TrainingArguments)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenize_func = lambda sentences: tokenizer(sentences['text'], \n",
    "                                            padding='max_length', truncation=True, max_length = 50)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_ds.map(tokenize_func, batched=True)\n",
    "val_dataset = validation_ds.map(tokenize_func, batched=True)\n",
    "test_dataset = test_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d28a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average='macro')[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.1,               # strength of weight decay\n",
    "    \n",
    "    logging_dir=r'C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\logs\\Original_model',            # directory for storing logs\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    "    #metric_for_best_model='f1',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f4e16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 840/1400 07:57 < 05:18, 1.76 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.779800</td>\n",
       "      <td>1.722508</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.234870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.565300</td>\n",
       "      <td>1.241647</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.551855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.706153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.347246</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.769917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.282225</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.769772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.885070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.268482</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.863137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.248575</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.881303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.888903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.262992</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.911562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.268907</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.937936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.907592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Aug 13 21:10:49 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-70\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-70\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-70\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\f1\\6a64529c5745513ceb230ce9696115dcad6ae0fedad9946e3f2f723a65a0cd53 (last modified on Sat Aug 13 21:10:53 2022) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-140\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-140\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-140\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-70] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Aug 13 21:10:49 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-210\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-210\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-210\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-140] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-280\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-280\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-280\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-210] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Aug 13 21:10:49 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-350\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-350\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-280] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Aug 13 21:10:49 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-420\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-420\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-420\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-350] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-490\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-490\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-490\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-560\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-560\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-560\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-420] due to args.save_total_limit\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-490] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-630\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-630\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-630\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-560] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-700\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-700\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-700\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\f1\\6a64529c5745513ceb230ce9696115dcad6ae0fedad9946e3f2f723a65a0cd53 (last modified on Sat Aug 13 21:10:53 2022) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-770\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-770\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-770\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Using the latest cached version of the module from C:\\Users\\DELL\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Aug 13 21:10:49 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-840\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-840\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-840\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-770] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\checkpoint-630 (score: 0.24374966323375702).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=840, training_loss=0.45793447295824685, metrics={'train_runtime': 480.0114, 'train_samples_per_second': 187.496, 'train_steps_per_second': 2.917, 'total_flos': 698608378800000.0, 'train_loss': 0.45793447295824685, 'epoch': 11.99})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training:\n",
    "trainer.train(ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "# We are going to get multiple loss values on each training step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8a1b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\trec_dataset_3runs\\models\\Original_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#save model:\n",
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f5350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14332188665866852,\n",
       " 'eval_accuracy': 0.97,\n",
       " 'eval_f1': 0.9553362604455932,\n",
       " 'eval_runtime': 43.3747,\n",
       " 'eval_samples_per_second': 11.527,\n",
       " 'eval_steps_per_second': 0.369,\n",
       " 'epoch': 11.99}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate:\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58749f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
