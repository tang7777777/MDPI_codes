{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c697ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\envs\\ta\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72de0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sms_spam (C:\\Users\\DELL\\.cache\\huggingface\\datasets\\sms_spam\\plain_text\\1.0.0\\53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label'],\n",
       "    num_rows: 5574\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 5574\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\DELL\\.cache\\huggingface\\datasets\\sms_spam\\plain_text\\1.0.0\\53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c\\cache-1dabca71be75cbef.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "dataset = load_dataset(\"sms_spam\")\n",
    "dataset = dataset.rename_column(\"sms\", \"text\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "processed_text = dataset['text']\n",
    "display(type(processed_text))\n",
    "processed_text = [s.replace('\\n', '') for s in processed_text]\n",
    "\n",
    "dataset = dataset.remove_columns([\"text\"])\n",
    "display(dataset)\n",
    "dataset = dataset.add_column(\"text\", processed_text)\n",
    "display(dataset)\n",
    "dataset = dataset.shuffle(seed=42).select([i for i in list(range(5500))])\n",
    "\n",
    "\n",
    "train_ds = Dataset.from_dict(dataset[0:4500])\n",
    "validation_ds = Dataset.from_dict(dataset[4500:5000])\n",
    "test_ds = Dataset.from_dict(dataset[5000:5500])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad687680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_ds)\n",
    "display(validation_ds)\n",
    "display(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95403ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 0, 0, 0, 0],\n",
       " 'text': ['sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p ',\n",
       "  \"It's justbeen overa week since we broke up and already our brains are going to mush!\",\n",
       "  'Not directly behind... Abt 4 rows behind ü...',\n",
       "  'Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?',\n",
       "  'Me too baby! I promise to treat you well! I bet you will take good care of me...']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ec0ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 1, 0, 1],\n",
       " 'text': ['Sent me ur email id soon',\n",
       "  'Lol no. Just trying to make your day a little more interesting',\n",
       "  \"Want 2 get laid tonight? Want real Dogging locations sent direct 2 ur Mob? Join the UK's largest Dogging Network by txting MOAN to 69888Nyt. ec2a. 31p.msg@150p\",\n",
       "  \"I'm still pretty weak today .. Bad day ?\",\n",
       "  'Moby Pub Quiz.Win a £100 High Street prize if u know who the new Duchess of Cornwall will be? Txt her first name to 82277.unsub STOP £1.50 008704050406 SP']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(validation_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af3c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 0, 0, 1],\n",
       " 'text': ['Yeah so basically any time next week you can get away from your mom &amp; get up before 3',\n",
       "  \"Haha... can... But i'm having dinner with my cousin...\",\n",
       "  'How is your schedule next week? I am out of town this weekend.',\n",
       "  'Alright i have a new goal now',\n",
       "  'Check Out Choose Your Babe Videos @ sms.shsex.netUN fgkslpoPW fgkslpo']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_ds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6644348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5267bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x000001503EDE7E50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 23.21ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.11ba/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#defining model:\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          Trainer, EarlyStoppingCallback,\n",
    "                          TrainingArguments)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenize_func = lambda sentences: tokenizer(sentences['text'], \n",
    "                                            padding='max_length', truncation=True, max_length = 100)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_ds.map(tokenize_func, batched=True)\n",
    "val_dataset = validation_ds.map(tokenize_func, batched=True)\n",
    "test_dataset = test_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d28a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average='macro')[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.1,               # strength of weight decay\n",
    "    \n",
    "    logging_dir=r'C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\logs\\Original Model',            # directory for storing logs\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    "    #metric_for_best_model='f1',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f4e16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1400 02:36 < 02:36, 4.46 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.406169</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.468650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.975801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.034847</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.980496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.980783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.980783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.027651</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.980783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.981061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.980783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.028136</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.985481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.030604</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.985481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-70\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-70\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-70\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-140\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-140\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-140\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-70] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-210\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-210\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-210\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-140] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-280\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-280\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-280\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-210] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-350\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-350\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-350\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-420\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-420\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-420\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-280] due to args.save_total_limit\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-350] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-490\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-490\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-490\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-420] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-560\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-560\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-560\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-630\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-630\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-630\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-560] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-700\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-700\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-630] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\checkpoint-490 (score: 0.025673454627394676).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.09158696155462946, metrics={'train_runtime': 158.1829, 'train_samples_per_second': 568.961, 'train_steps_per_second': 8.851, 'total_flos': 1164264246000000.0, 'train_loss': 0.09158696155462946, 'epoch': 9.99})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training:\n",
    "trainer.train(ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "# We are going to get multiple loss values on each training step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8a1b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\n",
      "Configuration saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\config.json\n",
      "Model weights saved in C:\\Users\\DELL\\Text_Augmentation\\sms_spam_dataset_3runs\\models\\Original Model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#save model:\n",
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f5350e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03539975732564926,\n",
       " 'eval_accuracy': 0.99,\n",
       " 'eval_f1': 0.9768241696099971,\n",
       " 'eval_runtime': 4.064,\n",
       " 'eval_samples_per_second': 123.032,\n",
       " 'eval_steps_per_second': 3.937,\n",
       " 'epoch': 9.99}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate:\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58749f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
